{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())           # ✅ True attendu\n",
    "print(torch.cuda.get_device_name(0))       # ✅ \"GeForce GTX 1650 Ti\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version : 2.4.1+cu121\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1) PRÉAMBULE ET IMPORTS\n",
    "###############################################################################\n",
    "\n",
    "# Pour affichage dans les notebooks Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports de base\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Imports des modules internes (à adapter selon l’organisation de ton projet)\n",
    "from train import Arguments, train, train_m_models\n",
    "from trainer_solution import train as train_loop_only\n",
    "from checkpointing import get_extrema_performance_steps, get_all_checkpoints\n",
    "from plotter import (\n",
    "    plot_loss_accs, \n",
    "    plot_scaling_results,\n",
    "    analyze_generalization,\n",
    "    plot_scaling_results\n",
    ")\n",
    "\n",
    "# Infos PyTorch\n",
    "print(\"PyTorch version :\", torch.__version__)\n",
    "\n",
    "# === Contrôle manuel du device ===\n",
    "# Pour forcer l'utilisation du CPU, passe `use_gpu = False`\n",
    "use_gpu = False\n",
    "device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Contrôle du seed (optionnel, mais recommandé pour la reproductibilité)\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training lstm (seed=0) ===\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sd \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m42\u001b[39m]:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (seed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m         mets, ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m         all_runs[(model_type, sd)] \u001b[38;5;241m=\u001b[39m (mets, ckpt)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m###############################################################################\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Résultats :\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m#  - On a des logs dans ./logs_better/<expName> \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Par exemple, on refait un plot \"manuellement\" pour l'un de nos runs \u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# (car train(...) appelle déjà plot_loss_accs(...) en interne).\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mrun_model_with_params\u001b[0;34m(model_type, seed)\u001b[0m\n\u001b[1;32m     52\u001b[0m args\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Lance l'entraînement\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m all_metrics, checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_metrics, checkpoint_path\n",
      "File \u001b[0;32m~/IFT6135_W25_A2_release/train.py:102\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, exp_param)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(args, exp_param: \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# Seed the experiment, for repeatability\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mseed_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     seed \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mseed\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# # Create a directory to save the experiment results\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# checkpoint_path = os.path.join(args.log_dir, str(args.exp_id))\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# i=0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m#     checkpoint_path = os.path.join(args.log_dir, str(i))\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# os.makedirs(checkpoint_path, exist_ok=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/IFT6135_W25_A2_release/train.py:33\u001b[0m, in \u001b[0;36mseed_experiment\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     32\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[1;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n",
      "File \u001b[0;32m~/miniconda/envs/torch_env/lib/python3.8/site-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torch_env/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda/envs/torch_env/lib/python3.8/site-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m~/miniconda/envs/torch_env/lib/python3.8/site-packages/torch/cuda/random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    124\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    125\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 127\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/torch_env/lib/python3.8/site-packages/torch/cuda/__init__.py:244\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m~/miniconda/envs/torch_env/lib/python3.8/site-packages/torch/cuda/random.py:125\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    124\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Exemple : Entraînement LSTM et GPT (2 seeds) avec paramètres plus régularisés\n",
    "###############################################################################\n",
    "import torch\n",
    "from train import Arguments, train\n",
    "from plotter import plot_loss_accs\n",
    "\n",
    "def run_model_with_params(model_type, seed):\n",
    "    \"\"\"\n",
    "    Prépare et lance un entraînement d'un modèle (LSTM ou GPT)\n",
    "    avec des hyperparamètres 'optimisés'.\n",
    "    \"\"\"\n",
    "    args = Arguments()\n",
    "\n",
    "    # Paramètres \"data\"\n",
    "    args.p = 31\n",
    "    args.operator = \"+\"\n",
    "    args.r_train = 0.5\n",
    "    args.operation_orders = 2\n",
    "\n",
    "    # Paramètres \"training\"\n",
    "    args.train_batch_size = 512\n",
    "    args.eval_batch_size   = 2**12\n",
    "    args.num_workers = 0\n",
    "    args.n_steps = 8000 + 1     # on réduit un peu pour éviter de trop pousser le surapprentissage\n",
    "    args.eval_first = 100\n",
    "    args.eval_period = 500\n",
    "    args.print_step = 500\n",
    "    args.save_model_step = 1000\n",
    "    args.save_statistic_step = 1000\n",
    "\n",
    "    # Modèle\n",
    "    args.model = model_type\n",
    "    args.num_layers = 2\n",
    "    args.embedding_size = 128\n",
    "    if model_type == \"lstm\":\n",
    "        args.hidden_size = 128\n",
    "    # Ajout de dropout\n",
    "    args.dropout = 0.2\n",
    "\n",
    "    # Optimiseur\n",
    "    args.optimizer = \"adamw\"\n",
    "    args.lr = 1e-3\n",
    "    # Weight decay plus élevé pour davantage de régularisation\n",
    "    args.weight_decay = 1e-2\n",
    "\n",
    "    # Exp & device\n",
    "    args.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    args.exp_name = f\"{model_type}_seed{seed}_betterReg\"\n",
    "    args.log_dir = \"./logs_better\"\n",
    "    args.seed = seed\n",
    "    args.verbose = True\n",
    "\n",
    "    # Lance l'entraînement\n",
    "    all_metrics, checkpoint_path = train(args)\n",
    "    return all_metrics, checkpoint_path\n",
    "\n",
    "###############################################################################\n",
    "# Boucle : on entraîne LSTM puis GPT, chacun pour 2 seeds (0 et 42).\n",
    "###############################################################################\n",
    "all_runs = {}\n",
    "for model_type in [\"lstm\", \"gpt\"]:\n",
    "    for sd in [0, 42]:\n",
    "        print(f\"\\n=== Training {model_type} (seed={sd}) ===\\n\")\n",
    "        mets, ckpt = run_model_with_params(model_type, sd)\n",
    "        all_runs[(model_type, sd)] = (mets, ckpt)\n",
    "\n",
    "###############################################################################\n",
    "# Résultats :\n",
    "#  - On a des logs dans ./logs_better/<expName> \n",
    "#  - On a tracé un PDF/PNG (si paramétré dans train(...)) \n",
    "#  - On peut re-tracer ici manuellement si on veut, par ex. pour comparer tout ensemble\n",
    "###############################################################################\n",
    "\n",
    "# Par exemple, on refait un plot \"manuellement\" pour l'un de nos runs \n",
    "# (car train(...) appelle déjà plot_loss_accs(...) en interne).\n",
    "from plotter import plot_loss_accs\n",
    "\n",
    "# Supposez qu'on veut re-visualiser LSTM seed=0\n",
    "lstm_seed0_metrics = all_runs[(\"lstm\", 0)][0]\n",
    "plot_loss_accs(\n",
    "    lstm_seed0_metrics, \n",
    "    multiple_runs=False, \n",
    "    log_x=False, \n",
    "    log_y=False, \n",
    "    fileName=\"LSTM_seed0_betterReg\", \n",
    "    filePath=None, # ou un chemin \n",
    "    show=True\n",
    ")\n",
    "\n",
    "print(\"\\n== Fini : Modèles entraînés avec dropout=0.2, weight_decay=1e-2, 8000 steps ==\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3) PARTIE 4.2 : Variation de r_train ∈ {0.1,...,0.9} avec des paramètres\n",
    "#    ajustés pour limiter le surapprentissage.\n",
    "###############################################################################\n",
    "\n",
    "def run_rtrain_experiments(\n",
    "    model_type=\"lstm\", \n",
    "    r_train_values=None, \n",
    "    seeds=[0], \n",
    "    dropout=0.2, \n",
    "    weight_decay=1e-2, \n",
    "    n_steps=8000\n",
    "):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle (LSTM ou GPT) pour plusieurs valeurs de r_train,\n",
    "    en appliquant un dropout plus élevé, un weight_decay plus grand, \n",
    "    et un n_steps moins élevé que par défaut pour limiter le surapprentissage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_type : str\n",
    "        \"lstm\" ou \"gpt\".\n",
    "    r_train_values : list\n",
    "        Les valeurs de r_train à tester (ex: [0.1, 0.2, ..., 0.9]).\n",
    "    seeds : list\n",
    "        Liste des seeds aléatoires.\n",
    "    dropout : float\n",
    "        Dropout appliqué au modèle.\n",
    "    weight_decay : float\n",
    "        Coefficient de L2 régularisation (AdamW).\n",
    "    n_steps : int\n",
    "        Nombre de pas d'entraînement (étapes).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Dictionnaire { (r_train, seed) : (all_metrics, checkpoint_path) }\n",
    "        all_metrics contient les stats d'entraînement,\n",
    "        checkpoint_path est le chemin de sauvegarde.\n",
    "    \"\"\"\n",
    "    if r_train_values is None:\n",
    "        r_train_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "    results = {}\n",
    "    for rtr in r_train_values:\n",
    "        for sd in seeds:\n",
    "            args = Arguments()\n",
    "\n",
    "            # Données\n",
    "            args.p = 31\n",
    "            args.operator = \"+\"\n",
    "            args.r_train = rtr\n",
    "            args.operation_orders = 2\n",
    "\n",
    "            # Entraînement\n",
    "            args.train_batch_size = 512\n",
    "            args.eval_batch_size  = 4096\n",
    "            args.n_steps = n_steps + 1  \n",
    "            args.eval_first = 100\n",
    "            args.eval_period = 500\n",
    "            args.print_step = 500\n",
    "            args.save_model_step = 1000\n",
    "            args.save_statistic_step = 1000\n",
    "\n",
    "            # Modèle\n",
    "            args.model = model_type\n",
    "            args.num_layers = 2\n",
    "            args.embedding_size = 128\n",
    "            if model_type==\"lstm\":\n",
    "                args.hidden_size = 128\n",
    "            args.dropout = dropout\n",
    "\n",
    "            # Optimiseur\n",
    "            args.optimizer = \"adamw\"\n",
    "            args.lr = 1e-3\n",
    "            args.weight_decay = weight_decay\n",
    "\n",
    "            # Nom de l'exp\n",
    "            args.exp_name = f\"rtrain_{model_type}_r{rtr}_seed{sd}_drop{dropout}_wd{weight_decay}\"\n",
    "            args.log_dir   = \"./logs_rtrain\"\n",
    "            args.seed      = sd\n",
    "            args.device    = device\n",
    "            args.verbose   = False  # Passez à True pour plus de logs\n",
    "\n",
    "            # Lance l'entraînement\n",
    "            mets, ckp = train(args)\n",
    "            results[(rtr, sd)] = (mets, ckp)\n",
    "    return results\n",
    "\n",
    "\n",
    "################################\n",
    "# EXEMPLE D'EXECUTION\n",
    "################################\n",
    "r_values = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "seeds    = [0, 42]\n",
    "\n",
    "# On applique dropout=0.2, weight_decay=1e-2, n_steps=8000\n",
    "res_lstm = run_rtrain_experiments(\n",
    "    model_type=\"lstm\",\n",
    "    r_train_values=r_values,\n",
    "    seeds=seeds,\n",
    "    dropout=0.2,\n",
    "    weight_decay=1e-2,\n",
    "    n_steps=8000\n",
    ")\n",
    "\n",
    "res_gpt = run_rtrain_experiments(\n",
    "    model_type=\"gpt\",\n",
    "    r_train_values=r_values,\n",
    "    seeds=seeds,\n",
    "    dropout=0.2,\n",
    "    weight_decay=1e-2,\n",
    "    n_steps=8000\n",
    ")\n",
    "\n",
    "print(\"=== ENTRAÎNEMENT TERMINÉ ===\")\n",
    "\n",
    "\n",
    "# Pour analyser et imprimer un petit résumé :\n",
    "from checkpointing import get_extrema_performance_steps\n",
    "print(\"=== RÉSULTATS LSTM ===\")\n",
    "for (rtr, sd), (mets, ckp) in res_lstm.items():\n",
    "    ext = get_extrema_performance_steps(mets)\n",
    "    print(f\"r_train={rtr}, seed={sd} => {ckp}\")\n",
    "    print(\"   - min_train_loss =\", ext[\"min_train_loss\"], \"at step=\", ext[\"min_train_loss_step\"])\n",
    "    print(\"   - max_train_acc  =\", ext[\"max_train_accuracy\"], \"at step=\", ext[\"max_train_accuracy_step\"])\n",
    "    print(\"   - min_val_loss   =\", ext[\"min_test_loss\"], \"at step=\", ext[\"min_test_loss_step\"])\n",
    "    print(\"   - max_val_acc    =\", ext[\"max_test_accuracy\"], \"at step=\", ext[\"max_test_accuracy_step\"])\n",
    "    print(\"\")\n",
    "\n",
    "print(\"=== RÉSULTATS GPT ===\")\n",
    "for (rtr, sd), (mets, ckp) in res_gpt.items():\n",
    "    ext = get_extrema_performance_steps(mets)\n",
    "    print(f\"r_train={rtr}, seed={sd} => {ckp}\")\n",
    "    print(\"   - min_train_loss =\", ext[\"min_train_loss\"], \"at step=\", ext[\"min_train_loss_step\"])\n",
    "    print(\"   - max_train_acc  =\", ext[\"max_train_accuracy\"], \"at step=\", ext[\"max_train_accuracy_step\"])\n",
    "    print(\"   - min_val_loss   =\", ext[\"min_test_loss\"], \"at step=\", ext[\"min_test_loss_step\"])\n",
    "    print(\"   - max_val_acc    =\", ext[\"max_test_accuracy\"], \"at step=\", ext[\"max_test_accuracy_step\"])\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4) PARTIE 4.3 : Mélange binaire/ternaire (p=11, operation_orders=[2,3])\n",
    "#    => Diviser le dataset manuellement (moitié binaire, moitié ternaire)\n",
    "#       ... puis entraîner LSTM et GPT ... \n",
    "###############################################################################\n",
    "\n",
    "import torch\n",
    "from data import get_arithmetic_dataset\n",
    "\n",
    "def get_mixed_bin_tern_dataset(p=11, operator=\"+\", r_train=0.5, seed=0):\n",
    "    \"\"\"\n",
    "    Extrait un dataset combinant eq_position=3 (binaire) et eq_position=5 (ternaire).\n",
    "    On veut un train avec ~50% binaire + 50% ternaire, \n",
    "    et un val set identique. \n",
    "    Cf. instructions de l'énoncé.\n",
    "    \"\"\"\n",
    "    from train import Arguments\n",
    "    args = Arguments()\n",
    "    args.p = p\n",
    "    args.operator = operator\n",
    "    args.r_train = 1.0  # On prend tout, on split manuellement\n",
    "    args.operation_orders = [2,3]\n",
    "    args.seed = seed\n",
    "\n",
    "    (full_dataset, _), tokenizer, maxlen, pad_idx = get_arithmetic_dataset(\n",
    "        p, p, operator, 1.0, [2,3], seed=seed\n",
    "    )\n",
    "    # eq_positions => 3 (binaire) ou 5 (ternaire)\n",
    "    idx_bin = [i for i in range(len(full_dataset)) if full_dataset[i][2]==3]\n",
    "    idx_ter = [i for i in range(len(full_dataset)) if full_dataset[i][2]==5]\n",
    "    bin_subset = torch.utils.data.Subset(full_dataset, idx_bin)\n",
    "    ter_subset = torch.utils.data.Subset(full_dataset, idx_ter)\n",
    "\n",
    "    # On fait un random split proportion r_train sur bin, r_train sur ter\n",
    "    # => r_train correspond à la fraction en train \n",
    "    from torch.utils.data import random_split\n",
    "    n_bin = len(bin_subset)\n",
    "    train_bin_len = int(r_train * n_bin)\n",
    "    val_bin_len   = n_bin - train_bin_len\n",
    "    bin_train, bin_val = random_split(bin_subset, [train_bin_len, val_bin_len], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    n_ter = len(ter_subset)\n",
    "    train_ter_len = int(r_train * n_ter)\n",
    "    val_ter_len   = n_ter - train_ter_len\n",
    "    ter_train, ter_val = random_split(ter_subset, [train_ter_len, val_ter_len], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "    # Combine\n",
    "    from torch.utils.data import ConcatDataset\n",
    "    train_dataset = ConcatDataset([bin_train, ter_train])\n",
    "    val_dataset   = ConcatDataset([bin_val,   ter_val])\n",
    "\n",
    "    return train_dataset, val_dataset, tokenizer, maxlen, pad_idx\n",
    "\n",
    "\n",
    "def train_mixed_bin_tern(p=11, operator=\"+\", r_train=0.5, seed=0, model_type=\"lstm\"):\n",
    "    \"\"\"\n",
    "    Entraîne un seul modèle (LSTM ou GPT) sur le dataset mixte \n",
    "    binaire/ternaire, p=11, r_train=0.5.\n",
    "    \"\"\"\n",
    "    from train import Arguments\n",
    "    args = Arguments()\n",
    "    args.p = p\n",
    "    args.operator = operator\n",
    "    args.r_train = r_train # mais on va l'ignorer pour data, on a un data custom\n",
    "    args.operation_orders = [2,3]\n",
    "    args.seed = seed\n",
    "    args.device=device\n",
    "    args.model=model_type\n",
    "    args.num_layers=2\n",
    "    args.embedding_size=64\n",
    "    if model_type==\"lstm\":\n",
    "        args.hidden_size=64\n",
    "    args.n_steps=5000\n",
    "    args.train_batch_size=128\n",
    "    args.eval_batch_size=512\n",
    "    args.log_dir=\"./logs_mixed\"\n",
    "    args.exp_name=f\"mixed_{model_type}_p{p}_seed{seed}\"\n",
    "    args.verbose=True\n",
    "\n",
    "    # On récupère un dataset custom:\n",
    "    train_dataset, val_dataset, tokenizer, maxlen, pad_idx = get_mixed_bin_tern_dataset(\n",
    "        p, operator, r_train, seed\n",
    "    )\n",
    "    # On construit manuellement DataLoader\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=args.eval_batch_size,  shuffle=False)\n",
    "\n",
    "    # On appelle la fonction train(...) du module trainer_solution, \n",
    "    # mais on doit nous-même créer le modèle, l'optimizer, etc. \n",
    "    # ou on essaie de réutiliser \"train(args)\" => alors on doit patcher\n",
    "    # train.py pour qu'il accepte un \"custom dataset\" en paramètre.\n",
    "    # Pour illustrer, on va faire \"train(args)\" et forcer la data \n",
    "    # (vous adapterez si besoin).\n",
    "    \n",
    "    # Trick: On modifie un petit peu \"train.py\" ou on fait un contournement\n",
    "    # direct pour appeler \"train_loop_only(...)\"\n",
    "    \n",
    "    # 1) On crée le modèle (LSTM ou GPT).\n",
    "    from lstm_solution import LSTMLM\n",
    "    from gpt_solution import GPT\n",
    "    vocabulary_size = len(tokenizer)\n",
    "    if model_type==\"lstm\":\n",
    "        model = LSTMLM(vocabulary_size, args.embedding_size, args.hidden_size, args.num_layers,\n",
    "                       dropout=args.dropout, padding_index=pad_idx,\n",
    "                       bias_lstm=True, bias_classifier=args.bias_classifier,\n",
    "                       share_embeddings=args.share_embeddings)\n",
    "    else:\n",
    "        model = GPT(num_heads=4, num_layers=args.num_layers,\n",
    "                    embedding_size=args.embedding_size, vocabulary_size=vocabulary_size,\n",
    "                    sequence_length=maxlen,\n",
    "                    multiplier=4, dropout=0.0, non_linearity=\"gelu\",\n",
    "                    padding_index=pad_idx, bias_attention=True, bias_classifier=args.bias_classifier,\n",
    "                    share_embeddings=args.share_embeddings)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # 2) Create optimizer\n",
    "    import torch.optim as optim\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "    # Dummy scheduler\n",
    "    from train import DummyScheduler\n",
    "    scheduler = DummyScheduler(optimizer)\n",
    "\n",
    "    # 3) Appel du \"train_loop_only\"\n",
    "    from trainer_solution import train as train_loop\n",
    "    # => train_loop(model, train_loader, train_loader_eval, test_loader, optimizer, scheduler, ...)\n",
    "\n",
    "    checkpoint_path = os.path.join(args.log_dir, args.exp_name)\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "    all_metrics = train_loop(\n",
    "        model, \n",
    "        train_loader, \n",
    "        train_loader,  # for eval on train\n",
    "        val_loader, \n",
    "        optimizer, scheduler, \n",
    "        device,\n",
    "        args.exp_name, checkpoint_path,\n",
    "        n_steps=args.n_steps,\n",
    "        eval_first=100,\n",
    "        eval_period=200,\n",
    "        print_step=200,\n",
    "        save_model_step=1000,\n",
    "        save_statistic_step=1000,\n",
    "        verbose=True\n",
    "    )\n",
    "    # On peut tracer\n",
    "    from plotter import plot_loss_accs\n",
    "    plot_loss_accs(all_metrics, multiple_runs=False, fileName=args.exp_name, filePath=checkpoint_path, show=True)\n",
    "    \n",
    "    return all_metrics, checkpoint_path\n",
    "\n",
    "\n",
    "# EXEMPLE D'UTILISATION:\n",
    "mets_lstm, ckp_lstm = train_mixed_bin_tern(model_type=\"lstm\", seed=0)\n",
    "mets_gpt,  ckp_gpt  = train_mixed_bin_tern(model_type=\"gpt\",  seed=0)\n",
    "# => Ensuite, vous regardez si binaire (eq_pos=3) s'apprend plus vite que ternaire (eq_pos=5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 5) PARTIE 4.4 : MISE A L'ECHELLE DU MODELE (L, d) => (L, d) ∈ {1,2,3} × {26,27,28}\n",
    "###############################################################################\n",
    "\n",
    "def run_scaling_experiments(model_type=\"lstm\", L_values=[1,2,3], d_values=[26,27,28], seed=0):\n",
    "    \"\"\"\n",
    "    Lance 3×3 expériences, pour L ∈ {1,2,3} et d ∈ {26,27,28}, \n",
    "    tout le reste inchangé (p=31, r_train=0.5, etc.).\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for L in L_values:\n",
    "        for d in d_values:\n",
    "            args = Arguments()\n",
    "            args.p=31\n",
    "            args.operator=\"+\"\n",
    "            args.r_train=0.5\n",
    "            args.operation_orders=2\n",
    "            args.train_batch_size=512\n",
    "            args.eval_batch_size=4096\n",
    "            args.model=model_type\n",
    "            args.num_layers=L\n",
    "            args.embedding_size=d\n",
    "            if model_type==\"lstm\":\n",
    "                args.hidden_size=d\n",
    "            args.dropout=0.0\n",
    "            args.optimizer=\"adamw\"\n",
    "            args.lr=1e-3\n",
    "            args.weight_decay=1e-3\n",
    "            args.n_steps=10_000+1\n",
    "            args.eval_period=500\n",
    "            args.print_step=500\n",
    "            args.exp_name=f\"scaling_{model_type}_L{L}_d{d}_seed{seed}\"\n",
    "            args.log_dir=\"./logs_scaling\"\n",
    "            args.seed=seed\n",
    "            args.device=device\n",
    "            args.verbose=False\n",
    "\n",
    "            mets, ckp = train(args)\n",
    "            results[(L,d)] = (mets, ckp)\n",
    "\n",
    "    return results\n",
    "\n",
    "# EXEMPLE:\n",
    "res_lstm_scaling = run_scaling_experiments(\"lstm\", [1,2,3], [26,27,28], seed=0)\n",
    "res_gpt_scaling  = run_scaling_experiments(\"gpt\",  [1,2,3], [26,27,28], seed=0)\n",
    "# => On récupère ensuite le nombre de paramètres (hors embeddings) \n",
    "#    et on trace la performance vs. P. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 6) PARTIE 4.5 : Variation batch_size B et T=2×10^4\n",
    "###############################################################################\n",
    "\n",
    "def run_batch_experiments(model_type=\"lstm\", B_values=[25,26,27,28,29], seed=0):\n",
    "    results = {}\n",
    "    for B in B_values:\n",
    "        args = Arguments()\n",
    "        args.p=31\n",
    "        args.operator=\"+\"\n",
    "        args.r_train=0.5\n",
    "        args.operation_orders=2\n",
    "        args.model=model_type\n",
    "        args.n_steps=20000+1\n",
    "        args.train_batch_size=B\n",
    "        args.eval_batch_size=4096\n",
    "        args.num_layers=2\n",
    "        args.embedding_size=64\n",
    "        if model_type==\"lstm\":\n",
    "            args.hidden_size=64\n",
    "        args.optimizer=\"adamw\"\n",
    "        args.lr=1e-3\n",
    "        args.weight_decay=1e-3\n",
    "        args.exp_name=f\"batch_{model_type}_B{B}_seed{seed}\"\n",
    "        args.log_dir=\"./logs_batch\"\n",
    "        args.seed=seed\n",
    "        args.device=device\n",
    "        args.verbose=False\n",
    "\n",
    "        mets, ckp = train(args)\n",
    "        results[B] = (mets, ckp)\n",
    "    return results\n",
    "\n",
    "# EX. => run_batch_experiments(\"lstm\", [32,64,128], seed=0)\n",
    "# Puis on analyse la performance en coupant l'entraînement à alpha*T steps, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 7) PARTIE 4.6 : Régularisation (weight_decay)\n",
    "#    => Ex: T=4×10^4 steps, wd ∈ {0.25, 0.5, 0.75,1.0}, et on trace la norme ℓ2 ...\n",
    "###############################################################################\n",
    "\n",
    "def run_weight_decay_experiments(wd_values=[0.25, 0.5, 0.75, 1.0], seed=0):\n",
    "    results={}\n",
    "    for wd in wd_values:\n",
    "        args=Arguments()\n",
    "        args.p=31\n",
    "        args.operator=\"+\"\n",
    "        args.r_train=0.5\n",
    "        args.operation_orders=2\n",
    "        args.model=\"lstm\"\n",
    "        args.n_steps=40000+1\n",
    "        args.weight_decay=wd\n",
    "        args.exp_name=f\"wd_lstm_{wd}_seed{seed}\"\n",
    "        args.log_dir=\"./logs_wd\"\n",
    "        args.seed=seed\n",
    "        args.device=device\n",
    "        args.verbose=False\n",
    "\n",
    "        mets, ckp = train(args)\n",
    "        results[wd] = (mets, ckp)\n",
    "    return results\n",
    "\n",
    "# => On pourra extraire la norme ℓ2 des paramètres si on modifie \"eval_model\" \n",
    "#    pour retourner la norme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 8) PARTIE 4.7 : Interprétabilité GPT (Poids d'attention)\n",
    "###############################################################################\n",
    "\n",
    "def visualize_gpt_attention(model, tokenizer, sequences):\n",
    "    \"\"\"\n",
    "    Montre un exemple de fonction pour visualiser \n",
    "    la matrice d'attentions (B, num_layers, num_heads, seq_len, seq_len).\n",
    "\n",
    "    'model' : GPT entraîné\n",
    "    'tokenizer' : votre tokenizer\n",
    "    'sequences' : liste de strings (ex: [\"[BOS] 3 + 4 = ?\"])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1) Tokenize\n",
    "    encoded_batch = []\n",
    "    max_len = 0\n",
    "    for seq in sequences:\n",
    "        # Supposez tokenizer.encode(seq) => liste d'IDs\n",
    "        # Vous adapterez selon votre tokenizer\n",
    "        ids = tokenizer.encode(seq)  \n",
    "        if len(ids)>max_len:\n",
    "            max_len = len(ids)\n",
    "        encoded_batch.append(ids)\n",
    "\n",
    "    # Padding\n",
    "    pad_id = tokenizer.pad_token_id if hasattr(tokenizer, \"pad_token_id\") else 0\n",
    "    batch_l = []\n",
    "    for ids in encoded_batch:\n",
    "        pad_len = max_len - len(ids)\n",
    "        new_ids = ids + [pad_id]*pad_len\n",
    "        batch_l.append(new_ids)\n",
    "    batch_tensor = torch.tensor(batch_l, dtype=torch.long, device=device)\n",
    "\n",
    "    # 2) Forward => (logits, (hidden_states, attentions))\n",
    "    with torch.no_grad():\n",
    "        logits, (hidden_states, attentions) = model(batch_tensor)\n",
    "        # attentions : (B, num_layers, num_heads, seq_len, seq_len)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    B = batch_tensor.size(0)\n",
    "    num_layers = attentions.size(1)\n",
    "    num_heads  = attentions.size(2)\n",
    "\n",
    "    for i in range(B):\n",
    "        # get the i-th item\n",
    "        att_i = attentions[i]  # shape (num_layers, num_heads, seq_len, seq_len)\n",
    "        # label tokens\n",
    "        tokens_ids = batch_l[i]\n",
    "        # decode if possible\n",
    "        tokens_str = tokenizer.decode(tokens_ids)  # ou autre\n",
    "\n",
    "        fig, axes = plt.subplots(num_layers, num_heads, figsize=(4*num_heads, 4*num_layers))\n",
    "        fig.suptitle(f\"Attention for sample {i}\")\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            for head in range(num_heads):\n",
    "                mat = att_i[layer, head].cpu().numpy()\n",
    "                ax = axes[layer, head] if num_layers>1 else axes[head]  # si single layer\n",
    "\n",
    "                sns.heatmap(mat, vmin=0, vmax=1, cmap=\"Blues\", ax=ax)\n",
    "                ax.set_title(f\"Layer={layer}, Head={head}\", fontsize=8)\n",
    "                # label x,y si vous voulez\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from checkpointing import get_all_checkpoints\n",
    "my_ckpt_dir = \"./logs_verif/verif_gpt_seed0\"\n",
    "all_models, all_mets = get_all_checkpoints(my_ckpt_dir, \"verif_gpt_seed0\", just_files=False)\n",
    "# supposez all_models[-1] = (last_model, last_step)\n",
    "gpt_model = all_models[-1][0]\n",
    "\n",
    "# Choisissez 2 échantillons, ex:\n",
    "seqs = [\"BOS 3 + 5 = ???\", \"BOS 7 + 2 = ???\"]\n",
    "visualize_gpt_attention(gpt_model, tokenizer, seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
